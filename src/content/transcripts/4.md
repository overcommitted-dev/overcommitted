Brittany Ellich (00:00)
Welcome to the Overcommitted Podcast where we talk about our commits, our commitments, and some stuff in between. I'm your host today, Brittany Ellick, joined by my fellow code enthusiasts, Bethany, Erica, and John. We're a crew of software engineers at GitHub. We first connected as an onboarding group on the same team in GitHub Actions and quickly discovered our shared passion for learning and building cool stuff.

Even though we're scattered across different teams now, we still come together regularly to geek out about technology, share what we're learning, and explore our life as developers. So whether you're committing code or committing to new challenges, we're glad you're here. Let's dive in. Today's episode is about how AI has transformed your work as a software engineer. Obviously, this is an evolving thing and subject to change even by next week.

when this actually airs. But I thought it would be a good opportunity to talk about what we're doing now with AI and what our thoughts are for the future of it. So to open up the conversation, I thought we could talk about what your current thoughts are about the AI landscape, just generally. John, do want to go first?

Jonathan Tamsut (01:10)
Yes, I can go. Wow, what a great question. What an interesting time to be alive. Today my mom was telling me how she used AI to solve a plumbing issue. But yeah, mean, you know, it's interesting. I think right like,

I think when, you know, my first use of chat GPT, and I think maybe a lot of people felt this way, was like, initially it was sort of just like a better interface for information retrieval, sort of like a better Google, like instead of returning, you know, 15, 12 blue links that you had to go click through, it kind of synthesized information and gave you more specific answers to the questions you had. I think now, right,

You know, now there's tools like GitHub Copilot that, you know, can, you know, I can write tests for you. I think, yeah, I mean, there's so much to say here. I mean, I use it every day. I'm sort of an AI maximalist. I'm like just constantly trying to see how much can I use AI to sort of automate my job and improve.

my efficiency, there's certainly limitations to this model as I'm sure we'll get to. I think one thing that I'm curious to hear what you guys have to say is there's also this idea of where do humans really fit within AI tooling? when you generate code with AI, the human still has to read it and validate it.

these things to hallucinate, they get things wrong. And it's interesting because it's like in 10 years, you if you think, if you sort of like think out 10 years ahead, it's like, and you know, if you're using these AI tools every day, what sort of what skills are you losing? What perspectives are you losing? I mean, I think all of us, you know, we're used to coding every day, keeping those tools sharp. So that's kind of just like a specific interesting thing I've been thinking about.

are we all just gonna forget how to code? But I think right now I'm definitely an AI maximalist and I'm just really interested in the upper bound of these tools, what they're capable of.

Erika (03:06)
Yeah, I think I'm in like a transition phase in my usage of AI where like, I think I was, so yeah, our organization is actually like providing us a dedicated week to like try out a bunch of things with AI, specifically co-pilot and really like upscale, which is.

important because like any tool you have to learn how to use it in order to get really any value from it. And I think the more that I do use it and the more I learn about it, you know, the more I feel confident in its use cases and applying it applying it correctly.

But yeah, I think something overarching that I feel is that the hard part will never go away, which is thinking through the problem. And even though when I'm using AI at its best, I feel like it turbocharges whatever I'm doing, the thinking part will never go away. For me, it's still...

my brain doing the hard work and then like telling AI, you know, giving a direction like, yeah, like I definitely don't see a world in which I'm going to offload that part. But it can be sort of like a second brain. Yeah. And like I said, kind of sometimes it feels like it's really.

Yeah, making my life a whole lot easier, making the things that I do a whole lot more efficient when I'm using it well.

Bethany (04:30)
Yeah, I definitely agree with all those points. I have found it the most useful to almost help with sorting through large volumes of information. So I find it super helpful for search. So anything that I would have gone to Google or Stack Overflow for, oftentimes I'll now go to like Copilot on GitHub.com.

and ask it that based on a repo or based on some documentation or a website or whatnot. So it can kind of be a more conversational approach to searching, which I really, really enjoy and feel like I learn things deeper when I can truly understand it. and it meets me at whatever level of understanding I'm at, whether I'm a beginner or more advanced. So I really think that's a huge.

pro to AI. I agree that I don't envision it ever fully taking over coding. I mean, after all, at some point an engineer or a person does have to support the code that's being deployed. Coding is only a fraction of the effort. But I do think AI can really help out with a lot of those issues, whether it's like finding where bugs might be or

understanding code if you're new to a language or a code base, like we talked about last week. And I think it's super helpful with that. And I think thinking of using AI tooling for coding as like analogous to pair programming is kind of how I go about it because I mean, you don't lose your coding experiences by pair programming with somebody but.

You rather, you know what you have in mind and you're mentoring and coaching them to that end product. And so that's kind of how I like to envision any coding sessions with AI going is, okay, I know what I want and I will like try to coach it through that, which I think there is some level of skill and that goes into from my experience before AI.

Jonathan Tamsut (06:26)
I think what Erica said actually was like super interesting. You're like the hard part is thinking and coming up with a solution. And it's like, you know, right? Like I think like large language models just like technically have a problem with like context and like sort of like thinking sort of like longer term thinking like with longer time horizons where there's like multiple steps.

But it is, I am interested in trying to use these tools and bringing in context and trying to get it to come up with end-to-end solutions. mean, just the other day I had it right. I had it refactor some tests and it got it right first time. I was using Claude, Sonnet 3.7. The test passed, I looked at them, they made sense to me.

And so like, yeah, it is interesting. Yeah. Like, sort of there's a gradient between like, I'm using, I'm using AI to kind of figure out an API call to I'm pair programming to I'm giving it, I'm telling it to write tests to like some agentic, you know, I think dream where like you're, you give it a JIRA ticket or a copilot, you know, or a GitHub issue or whatever. And it can bring in multiple sources of information, maybe ask clarifying questions and just like implement it end end.

Erika (07:45)
Yeah, think like the way I think about it too is like I need to have a mental model of whatever I'm doing in order to be effective in that space, which is kind of something we talked about last week too with the...

like learning a new repository or a new problem space. And like, you have to be able to like hold that in your mind to know if like any of the output you're getting from the AI is even applicable. Like you said, like the tests made sense and they have to make sense compared to what you're expecting in your mind, right? But yeah, I think like we've probably all had this experience where like,

One of the most frustrating things as a software engineer is not when you're in the flow and coding, but when you have that one issue that takes you five hours that could have taken you five minutes. And like, those are the things that I feel like AI is so good for, where it...

immediately sees like you have this dumb typo, you have this thing that's not passing in your tests and you're like that was so obvious but like for some reason my you know eyes didn't catch it so yeah those are those are huge productivity wins.

Brittany Ellich (08:53)
Yeah, for sure. I, I found, think I started out as like, not quite an enthusiast. Like I'm pretty slow on the adoption curve for AI in the last month or two. I've started realizing like, wow, this is going, this is improving a lot faster than technologies normally do. and now trying to really rethink how to add this to my workflow. And I'm thinking about it a lot because your entire workflow really does change. If you're used to going to Stack Overflow for, or Google or something like that, like that.

process is different now when writing code so it's a very fun time and also like a slightly uncomfortable time while we're trying to like relearn how to work. Love what you all have said and I feel like we have like a good range from an AI maximalist down to you know I haven't used it ton and I feel like I'm starting to now and it's it's been really interesting.

So let's talk about tools that you are using for AI at work and the ones that we can talk about given that we all work at a company that makes AI tools. I think we have to be a little bit general about some stuff, but we can talk about using Copilot and stuff like that. let's start first with documentation and writing. What tools are you using for that specifically?

Bethany (10:05)
I will a lot of times use Claude for documentation and writing. I recently got a Claude.

subscription because I was, as mentioned in a previous episode, I was trying to draft a bunch of proposals for GopherCon and was just looking for help to make sure that I was drafting it in a way that matched what they were expecting based on the resources they provided. And I thought the projects space was really cool to be able to

add any documentation or frameworks or whatever you're looking to use as context and be able to chat within those parameters. And I think OpenAI has its own version of that as well. And so I find that really helpful for just kind of brainstorming and getting ideas. For writing documentation in general, I'll usually write a loose

rough draft of what I'm looking for if I already know what I want to say. And I often will check it against like cloud or if it's more technical against co-pilot in in github.com to like make sure it makes sense, it flows well. And I'm conveying those my thoughts the way that other people can understand them. And I find it super helpful for just making sure that

what I'm saying makes sense because sometimes it's tough if you have a lot of knowledge on something and you're just brain dump. People are like, I don't understand any of this. So I really appreciate that.

Erika (11:35)
Yesterday, for the first time, I used the, I think it's slash doc command in Copilot Chat, which will generate documentation, code comments for a certain set of methods or the whole, all of the methods in a file. And that...

is really nice for anyone who's reading your code or even if you're looking at a method that you wrote and trying to make sure it makes sense, like inputs and outputs. that's, I think that's a nice feature that's built in now.

Jonathan Tamsut (12:11)
Yeah, that's super cool. I actually haven't used that. So I will check that out. Yeah, I mean, I feel like I use tools. like, you know, I use like, I like a Copilot Pro and Cloud Pro account. And I use them a lot for like editing. I'll paste in something I wrote, like for personal project, like a blog post. I'll paste in a blog post and say, can you edit this for clarity?

I've used Notebook LLM, which is Google's open source tool, which I think you get similar functionality using Claude. I'll upload it. I wanted to write a paper, or I wanted to write a blog post on a paper. So I uploaded the paper, and I had Notebook LLM summarize the paper for me. And then I asked it to write me a little blog post, kind of rough draft.

Yeah, so I think, you know, I'm actually, I'm not sure if this is released yet, but I mean, the other thing I use a lot, sorry, sorry for making your editing job life harder. But I know like Copilot also generates PR comments, which sometimes when I generate those, that's nice. Or sorry, yeah, like.

PR descriptions. And then yeah, yeah. Sorry, I gone.

Erika (13:16)
Yeah. And

merge commit messages. Those are so nice.

Jonathan Tamsut (13:21)
Yes.

Totally. And it's kind of like, I, you know, it's funny, like I write pretty poor merge commit messages. And it is kind of nice like having that automated. Are they always the best? No. Are they a little wordy? But you know, you can also tune that, you know, you could tune that. And then yeah, like Gemini, Gemini is pretty useful and like the G Suite. You know, I'll just reach for that.

One thing I've really gotten into is I like generating header images for my Google Docs. So you can generate a picture of databases and then flames, and you can have a fun header image. So yeah.

Brittany Ellich (14:02)
Is that like a good database paper or a bad database paper if it's on fire?

Jonathan Tamsut (14:06)
No, no, that's

like that's like when I'm when I'm writing like an availability review postmortem. Yeah.

Brittany Ellich (14:12)
Gotcha.

That makes sense.

Yeah, I go back and forth between Claude and ChatGPT right now. I heard a really funny joke the other day where folks were talking about how they're spending more right now on their AI subscriptions for different things than they are on their streaming service subscriptions, which might just be the state of where we're at. I haven't committed to one just yet to subscribe to, but I'm getting there. I really want the project's functionality, which I think I have to subscribe to Claude or to open it, or to ChatGPT to be able to get, so.

Still waiting to see which one wins between those two. I've also been like a googling what's on our public docs while we're talking about this to see like what has been released because we do get to you know do a lot of dog fooding and try stuff out before it gets its public so I think we're good on PR descriptions. My brother just mentioned it the other day so I think we're safe there.

Nice. What about for code? Obviously, you know, is there a different workflow for code? We're all obviously using a lot of copilot. But is there anything that you want to talk about there in terms of, you know, different things that you're using?

Jonathan Tamsut (15:12)
Well, so I listened to this podcaster, Lex Friedman, shout out Lex Friedman. He interviews people and he had the cursor team on. And you know, it's these four like MIT graduates and they spent a lot of time talking about sort of like the UX of these coding tools, these AI coding tools. And you know, it's like, I think,

I don't think the user experience is exactly like, I don't think it's where I would want it to be. And I think one thing that I think about a lot is, I used to use NeoVim and Vim, and now I have Vim bindings. And one of the nice things about that is if you use it a lot, you eventually get to a point where editing a document, you're not using brain cycles for that. instead you can think about the code you're trying to write. And I think like,

you know, some type of seamless integration with my text editor would be nice. I think like sometimes, you know, auto completions can feel a little aggressive, you know. So I think something that would like really almost like, almost like predict what I wanna do and really just allow me to like, you know, seamlessly edit things very quickly and kind of.

Yeah, almost like predict where I'm gonna miss something would be nice. And yeah, you don't want auto completes that are aggressive or incorrect, sometimes I accidentally hit tab and then I have to go and delete four lines of code, which is kind of frustrating. So I think there's, I mean, obviously we're early days with these tools and I think that eventually there'll be better UX.

really kind of augments human behavior in a more seamless way.

Erika (16:45)
Yeah, I mean, I'm excited because I think working on the GitHub monolith is a lot of discovery, discovery of style, best practices, discovery of tooling. there's a lot to grok before you get to a point where

you're writing one line of code. And again, that to me has always been the harder part. And like we said, that's something that I think that AI is really good at is crunching large amounts of information. I haven't gotten to the point where I've set up enough customization and context to really get

a ton out of it, so I'm still in the kind of like playing and experimenting phase with like different prompts and all that, but I think there's a ton of potential there. And then I've also seen some really cool experiments this week around using MCP servers to pull like Sentry and Splunk and Datadog.

metrics and help troubleshoot different problem areas in the code, which is super cool and something that also takes a lot of time. It's like digging through all of those errors to try to find the problem area. So again, I have not tested out a ton of that myself, but those are kind of the things that I'm excited about potentially incorporating into my workflow. I'll keep you posted on.

How it

Bethany (18:20)
I personally use Vim, like new Vim, a lot. So I do rely on completions a ton for helping out there. And if I need to ask a question or about code, then I'll go to github.com. I do use VS Code a lot though to triage Copilot API, which I work on. So...

I am really excited to actually learn more about the agent mode and how to actually prompt it in a useful way to get to get a good outcome. But I've heard a lot of really good stories about that. So very excited about that. And also definitely want to echo Erica that MCP, it's still early days, but lots of exciting things coming from that. And a lot of really interesting.

use cases for it to really make a cool AI experience that can leverage your tools and almost like read your mind and be like, okay, these are the tools I would use go, go do what I would typically do, but in less time. very excited for that.

Jonathan Tamsut (19:19)
Yeah, and to your point, Erica, I think kind of the next level is having an AI have context across a variety of information silos. So not just your code, but your code running in production, the different telemetry. I think that'll help with triaging, that'll help sort of connecting what you're seeing.

Was your code running under load in production to the code you're writing now that'll help discover bugs? I think that's something that definitely seems like an area of growth and pretty exciting. If you could ask, hey, I have this query and the AI knows about your database, the load it's under, different metrics about your database, knows telemetry. You could say, is this query going to perform well under load? And it can answer questions like that.

That'd be pretty neat.

Erika (20:10)
Yeah, and it's something that I feel like that specifically we reach a lot for linters, which are a very imperfect tool because linters are basically like pattern string matching. But like if we can turbocharge that and say like, hey, these are like certain code patterns that we've noticed have caused like availability incidents in the past, like, you know, like that's so much more powerful than saying like,

your code matching this one specific thing that we have now thought of, any sort of predictive error analysis, not necessarily to prevent it, but to flag it and say, hey, have you thought about this? Batching might be a good idea here. Something like that. So yeah, I think that be really cool.

Jonathan Tamsut (20:55)
There has to be startups doing this, but some agent that's listening to your telemetry, ingesting it, knows is aware of your code base, is aware of the infrastructure your code runs on, and then can propose solutions like, you've had this exception happen 3,000 times in the last month, here's how to fix it. mean, that would be incredibly useful.

Erika (21:14)
Yeah, I almost wonder about what the output there would be. Would that be code, or would it be a rule set? And how you would apply that. So I'm not sure.

Something interesting to think about.

Brittany Ellich (21:27)
I feel like this is a topic we're going to have to continuously revisit as things change because it is changing so fast and it's fun to kind of hear some of these ideas and predictions about the future. Is there anything like what is the thing right now that you're most excited about seeing develop further?

Erika (21:44)
Well, I think the flip side of my excitement about MCP is a lot of the security vulnerabilities that have been brought up. So I guess this is an excitement of the potential of those getting fixed, but also a fair warning that.

I read something this morning that over 43 % of MCP server implementations have unsafe shell calls. Yeah, and they're like a major potential vector for supply chain attacks. you know, it's like, beware, be cautious of these things. But I think like, you know, it's not impossible to harden them and make them.

better. So like, I'm excited to see that and I'm, yeah, like I'm sure it'll, it'll, it'll get better.

Jonathan Tamsut (22:31)
And for me, I'm excited about, you know, sort of agents, I mean, that's kind of where everything is going right now. you know, what level of human involvement will be required? There's a lot of tasks that are trivial, that take time, that take brain space. It would be nice to just tell, you know, go change this config here. It would be nice, you know, I think like one thing that I've been wanting to try is like,

automate testing, right? I mean, I think like, I think AI could do a good job testing, you know, if you tell framework, you can say, you know, as I'm typing, generate tests, run the tests, even if you could automate testing, like, sort of like in a more sort of procedural way, like SSH into this, into this box, run this command, and tell me what the output is. These are all kind of just like, boring tasks that I would love to

have an AI do for me.

Brittany Ellich (23:18)
I see this. is the new A.I. TDD from Jonathan Tamsut A new paradigm around writing tests.

Jonathan Tamsut (23:25)
Yeah, for sure. I've seen them. I have a bunch of blog posts bookmarked where people are integrating open APIs into their testing. So I think a nice framework around that would be nice.

Bethany (23:38)
I think for me, I am really excited that a lot of new models that we're seeing are getting more and more efficient, which will just make things one, more friendly for the environment and two, more sustainable because when LLMs first came out, they definitely were not sustainable and capacity was very difficult to get and costly. So I'm super excited about that.

we're going in the direction where they've become so much more efficient and energy friendly. Additionally, I'm super excited that reasoning models seem to be the new norm now. So previously there would be models that, like a lightweight model such as like 3.5 turbo or 4.0 mini and then like a heavier model like GPT-4, GPT-4.0.

But a lot of times, now they're pushing reasoning models where you could optionally provide reasoning parameters. So you can really use the same model, but tell it how hard to think on a problem, which I think is a really interesting way to think through leveraging these models and LLMs. That's really interesting.

Jonathan Tamsut (24:43)
Yeah, like the think deeply. It's like, yeah, it's like how much do I sort of care about a problem is how much I'll ask the AI to think about it. Yeah, that's cool angle.

Bethany (24:54)
Real quickly, I wonder if it would be beneficial to define MCP and give a quick overview for any, listening, that might not have a good understanding of

Brittany Ellich (25:02)
Gosh, yes please.

Please, please do. Cause I still, I've tried to like address this so many times, like, okay, I kind of get it, but please from the expert, tell me what is MCP.

Bethany (25:12)
Am I the expert here?

Brittany Ellich (25:13)
You're like the closest to AI, I feel like.

Jonathan Tamsut (25:15)
Yeah,

you work on copilot. You're the expert. Yeah.

Brittany Ellich (25:19)
Yeah.

Bethany (25:21)
Well, my understanding of MCP, MCP means Model Context Protocol, and it is basically an API to provide a LLM so that it knows what tools it can use for its processing. if it comes to a

answer or a question that it's not sure about, it can see what's in its toolset, which MCP describes, and it can go out and then fetch what it needs to actually answer a question. So you'll see a lot of, like GitHub just announced an MCP, which has like a lot of GitHub functions that you can provide. And Erica was mentioning a lot of really cool implementations with like spunk, I think, and sentry.

And then even I saw one for Obsidian, which I'm really excited to tinker with and see what I can do with that. But it's still very much early days, so I think it's very much evolving still. But a good analogy to it that I've heard is like LSPs. So I think that's language server protocols. If anyone, if it's otherwise, correct me. where...

people can define what language structure is and then IDEs can pull that and be able to show the nice colors and underline errors and things like that. it's meant to kind of be like that except for LLM tooling.

Brittany Ellich (26:47)
So just to, so it's kind of like you're giving it all of these extra things that it can pull context on if it needs it. Okay, so it's like extra API is that it has available to it to get more context if it thinks that it needs to know more about something.

Bethany (27:01)
And it's kind of nice because previously, so with agents, for instance, if you have multiple agents in the same conversation, there could be crosstalk between different things. take the sentry and splunk example, for instance. If you ask splunk for something and it pulls logs and then sends that to sentry, that could be not great for data crossover. So MCP, I believe, should be a little

better about keeping things client side so that it's a lot of the context isn't going to these other tools and rather keeping separate.

Jonathan Tamsut (27:35)
Yeah, that's interesting. I think, Bethany, was helpful. I think like one question I do have is like, you know, so like you're using some like base foundation model that knows, has sort of a general amount of knowledge, was trained on some general corpus, but like I wonder how MCP works with like ingesting splunk logs. Like I guess like maybe the underlying

model has knowledge about what a log is, but how does it necessarily know what a Splunk log is or how to reason through these? Because is there some post-training task you can do on top of MCP to make it work better with a specific tool? I wonder how that works.

I don't know if anyone has an answer to that. That's just a question I have.

Bethany (28:17)
That's probably the boundary of my knowledge in terms of how it's actually used in practice. But I do know with regular tool calls, which I assume it would end up being something similar to this, you provide a description for what a tool call is and does and then kind of the parameters it needs so that it gives your

LLM more of an understanding for how to use that tool. And so I think there's a level of skill in prompting that correctly and making sure that it knows when it should be using those tooling calls effectively.

Jonathan Tamsut (28:49)
Yeah, and plug for that book by Dorkesh Patel that I linked yesterday. Actually, I'm forgetting the title of the book. Maybe we'll put the title in the show notes. Maybe not. If it's there, then you'll see it. Dorkesh Patel is pretty cool. If you haven't listened to him, he has a podcast. He interviews people, and a lot of them are...

AI people and so his book was a pretty good like I started reading it was like a pretty good overview of like sort of like philosophical topics and LLMs and then just also taught like more practical stuff like how to

recommended.

Brittany Ellich (29:19)
That's

cool. I definitely feel a little bit more pressure now from the industry in general to learn more about this and, you know, see where the future of this goes. This was great. Thank you all. This feels like our normal sessions where we're just sharing things that we've learned about, which is fun. I'm sure we'll come back to AI in the future, like we have talked about AI a lot in the past.

How about we start wrapping things up and talk about one app idea if you are not already committed, something that you would be exploring right now with AI if you are not already committed, over committed.

Erika (29:53)
So speaking of like knowledge bases, I use Slack a lot as a knowledge base. And I do have Slack AI enabled, which is nice. And when I search for something, it provides suggestions or like, you know, a variety of results. But I have found it.

pretty limited. And I think something that I would love to build is a more complete search result page for Slack, including like suggested keywords, like suggested alternate searches, ranking by relevance or like time.

And then also I have this like idea of like a visualization or there's like some kind of like heat map by channel of like how often something is mentioned. Yeah. So that's what I would be building is like a supercharged Slack search interface. My dream world.

Jonathan Tamsut (30:55)
And I love that idea and one of my dreams is to kind of like sort of to piggyback off that is like you know there's information silos and like you know you work at a company there's just so many so much time you're just searching for stuff and so some yeah some type of like LLM interface that you know integrates your internal docs or your code issues PRs everything in slack I mean

That would be super useful. So, you don't know, you know, that's a pretty ambitious app, but like, I think that like, with LLMs, training on sort of the total sum of internal documents that you'd be interested in would just be incredible.

Brittany Ellich (31:30)
To build on that, think I would build, I wanted to experiment with some of the Obsidian LLM stuff and I feel like if there was some connection, if we could just merge all the things together in the world, all of the knowledge, then that would be great, obviously. But if there was some sort of a connection between the notes I've taken as well as the internal documentation, maybe that's just a sign that I need to write more internal documentation so that it can all be included in that. yeah, I really would like to expand

experiment more with the obsidian stuff.

Erika (31:59)
that

would be like especially cool if there was some kind of like push functionality where like it would like suggest things in your notes like it was like searching things and being like I found this potential inaccuracy or like I found this related post that like might update whatever information you had.

Yeah, again, dream world.

Bethany (32:17)
I think you, all should just quit your day jobs and build these things for me because I want all of these amazing ideas. My idea, well, so last week I pitched my idea and John had a solution to it. So maybe it'll happen this week if I pitch this other idea. But every Christmas my family does Christmas lists.

for like what they want and stuff and without fail we- so they only talk with each other on Snapchat. So without fail we have a bunch of sub-Snapchat groups with just everyone but one person say, okay what are you getting, what are you getting, what are you getting? I would like to create an app that is basically a wedding registry except for just-

Christmas or birthdays or whatever holidays and then other people can see what other people are like committing to and then the person who made the list doesn't see it. But that would be my dream.

Erika (33:15)
I'm like commenting on everybody's ideas, but like I think one fun alternate to that is like an evil alternative, right? Where it's like, it like sees what you get and like gets you the opposite, like suggests like... Yeah, yeah, exactly.

Brittany Ellich (33:30)
Like a white elephant one version of it. Yeah, the gag gift

version. That would be good. Yeah, I like that idea. I like that idea a lot of I'm terrible at picking out gifts. So and I think about it way too much. So I would definitely use that.

Bethany (33:44)
Well, maybe I'll vibe code it one of these days. It's like just, it's simple enough where I'm like, I maybe could do it, but complicated enough in terms of you'd have to have users and like a server that it's, I haven't set aside the time to do it yet.

Brittany Ellich (33:47)
There you go.

Yeah, well maybe next hackathon we can work on that. That would be fun.

Awesome! Well, thank you all for tuning in to Overcommitted. This has been a really fun conversation. I love talking about the state of things as it relates to AI, especially because it's changing so quickly, like weekly almost. I don't think I heard of MCP before like two weeks ago and now it's like literally everywhere. So I'm glad that I finally have a definition. If you like what you hear, please do follow or subscribe or whatever it is you're supposed to do on whatever your podcast app of choice is.

also

now on Blue Sky which is our social media app of choice. I unilaterally made that decision because that's the one that I use the most and made everybody else make an account so we're all there now. Most of all I think the most important thing is just share with your friends until next week we are over committed and thank you for hanging out.

