Jonathan Tamsut (00:00)
Hey, what is up? Welcome to the Overcommitted Podcast, where we talk about our code commits, our personal commitments, stuff in between, and just generally have a good time. I'm your host, Jonathan Tamsit. I am joined by...

Brittany Ellich (00:14)
I'm Brittany.

Erika (00:15)
And I'm Erica.

Jonathan Tamsut (00:17)
We are software

engineers. initially met as new hires at GitHub and we found a common interest in continuous learning, building interesting projects, and just being a human in the tech world. We continue to meet to talk about stuff we've experienced and discuss our lives in this crazy world called the tech world.

whether you're pushing code or taking on new challenges, we're happy you're listening. And today we're gonna talk about a pretty trendy topic, AI agents. So I think this is something that we're all kind of exploring and I think is kind of new to the industry. And yeah, we just wanna kind of, I guess, learn and I think this will maybe be like a learn in public type conversation. And...

we'll kind of see where this goes. So I guess to kind of start us off.

why are AI agents exciting to you? Why are they interesting to you? I mean, I think there's kind of like the obvious reason I think for me is like, you know, we have these,

tools, these large language models, and they're capable of information retrieval and generating text and reasoning. And I think that there's a bunch of jobs that humans do that are boring and that humans don't want to do. And I think it's just interesting to see how can you take these building blocks of AI tooling to develop these

you know, ⁓ agents or whatever, these entities that kind of are able to kind of perform these tasks. And I think like, you know, kind of, you know, kind of in my dream, like a lot of these tasks, like it's like human out of loop, like humans aren't involved at all. But yeah, I guess, yeah, like, I don't know. Yeah, why are AI agents interesting to you? Like when you guys hear the word AI agent, kind of what about that excites you? I'm just curious.

Kind of a vague question, but.

Brittany Ellich (02:15)
I can go first. So for full disclosure, the extent of my experience with AI agents is really using tools that are already pre-built for this, like the GitHub Copilot Coding Agent. That one was really nice to use because I can just start using it with a GitHub issue, and that was not outside of my comfort zone in terms of things that I was already doing. But I am very interested in learning how to build my own agent, and I think...

I think I find it really interesting because I think it's a new way to think about solving problems. You know, previously it's about like, you're going to build this deterministic route of like this to the API to this, to like gather stuff. But then you're going to have to do a lot of thinking about the data that you get back and decide what to do with it. And the idea is with an AI agent, can theoretically offload some of that thinking. So thinking about this.

episode and preparing for it I was thinking about okay what is a practical problem that I could try to solve with an agent and I came up with a whole bunch of them and so now I'm really excited to learn how to build them and learn what to do with them so that I can ⁓ actually partake in this trend.

Jonathan Tamsut (03:07)
Mmm.

Erika (03:19)
The thing that interests me the most is the breakdown of the problem. I've similarly only used GitHub Copilot Agent Mode, but I find it really interesting how it breaks down the problem, which is a critical part of building an agent is understanding how to direct the model or the agent to.

like take sequential actions and use like outside resources. it's second brain to the like fullest degree where you have to like, like you have to know how you would solve the problem. And sometimes it's not always the same like the way the agent does it and the way that I would do it. So I think that's a, it's an interesting.

way of thinking of putting on my computer hat and thinking how I would think about something as a computer. And it's kind of a true truism for all software engineering, but it's a systemized version of it versus individual problems. And like thinking of how you achieve an end goal.

Jonathan Tamsut (04:25)
you

Erika (04:28)
versus solve like one specific problem.

Jonathan Tamsut (04:31)
Yeah, I think, know, so I kind of like proposed the topic of talking about agents. I mean, I think we're all interested in it. But one of the things that was kind of inspiring to me was there's this venture capitalist, name is Sarah Guo. She started a VC firm called Conviction. And she has a list of

startups that are like potential ideas and she also gave a talk and I'll link the list of serves and talk and the show notes, but I think great like I think one thing she said was, you know,

You know, a few things. One is like, you know, I think that like we have to fundamentally rethink the user experience with agents, right? So I think like software, you know, we have tables and infinite scrolls and there are these like primitives that exist currently with like modern software and you have, you know.

You know, the paginated table or the infinite scroll or whatever, and users are clicking buttons and reading things. You know, you have, you know, your 10 blue links, whatever, with Google. And right, like I think with agents, I think that like, and I don't know, I don't know what the, the best interface is, but I think there's a different interface that exists. like, for example, like I was listening to podcasts with the guys who created Cursor.

talking and they were saying that you know one of their, you know, in a few views cursor, this actually works really well, they have this feature where when you're done typing it will predict where you would want to add code next and take you there. So it has this, this is like a small UX thing, if you're like typing in a file, you know, let's say you like update a variable name, it'll say okay like now do you want to go to this file and update the variable name.

And it's like this like small UX thing. And they were talking about like, you know, they have like a model that basically outputs, you know, it predicts where you're going to go next. And so I think there's just like, I think there's like a different way of us interacting with, you know, technology. mean, another example is like, instead of going on and booking a flight, you know, maybe there'll be an agent where you just describe the kind of constraints you have and it'll go off and do it. Right. And so I think like,

Yeah, so Sarah Bow talked about this. The other thing she talked about was like, you know, like we've talked about previously this like dream of artificial general intelligence. And I think like, you know, in the short term, you know, maybe before AGI has reached or if it's ever reached, right, there's gonna there's like, this incredible opportunity for people who are like domain experts in something to build agents that like perfectly solve a problem for a certain

type of person. So you can think of like support customer, know, support staff, you know, that's an example. But like, I don't know, if you're like an expert in construction or whatever, like there's, there's, think there's like a ton of start ups, just like we're building the cursor for X, we're building the cursor for construction, we're building the cursor for help, you know, for, nurse skilled nursing facilities, et cetera. And I've like seen a lot of startups that are being funded for this. So I think, I think like, you know, agents,

like a fundamentally new paradigm for writing software and a new interface. And I think that's like interesting to explore. It's kind of the TLDR I ramble. Okay, any thoughts or I guess we can, if not, can kind of, I guess talk about, yeah, maybe we can like just define what we mean by an agent and you know, maybe,

if there's anything that kind of, you know, sparks your interest there. So I guess, ⁓ kind of just kind of setting some context. so, you know, agent is like, yeah, like in general, an agent is, it's kind of like a term that is, I think, existed in artificial intelligence for a while, but in our context, it's like, there's kind of three things an agent does. It...

perceives some input. an agent can either receive text or can make an API call or whatever. It uses that to plan its next action. It acts. And so that action can be calling a tool or firing off, doing something or responding to a user or something. then you kind of have this chain of like,

perception, deciding what to do acting, and then kind of like starting over and over again. We've talked about previously agents often have the ability to use tools. And I think agent is kind of an overloaded term. Like the quote unquote agents that I've kind of experimented with have been just like basically continuously calling.

you know, the open AI API. like, you know, like I, I, built, I built, I built an agent, you know, or I built like a Python script to get my emails and like help me decide, ⁓ you know, how do, how to like, you know, respond, you know, help sort my emails in terms of, you know, priority items. I also said to myself, like to do items via my emails. I wanted the LLM to like,

grab my to-do items, sort them between personal and work, and then list them. And they're sort of unstructured. So I guess, I think one question for you guys is, when you think about an agent, what do you guys think about? Thinking about that definition, what to you as an agent? And I know there's a lot of

of definitions here but I'm just curious like what your understanding or opinions or whatever.

Brittany Ellich (09:54)
My thoughts are that it's just like a programming loop, like you would have for any other thing that has previously existed, but it involves AI at some point or LLMs at some point in the process where one of the steps that it's involves that. I think the...

Jonathan Tamsut (09:58)
Yeah.

Brittany Ellich (10:12)
Important thing is that those loops have existed, being able to generate your own loop that will go through and do a task for you has existed for a very long time. And this is just applying that same concept to using LLM tools as part of it.

Jonathan Tamsut (10:25)
Yeah, yeah, and there's like a bunch of, I think Anthropic has, and I can share another, I came across a while ago, like, yeah, I mean, I think, right, yeah, like fundamental to like core pieces of an agent or like the model, models. One thing we can talk about is, you know, these, like, there's kind of like different layers in which these models, you can inject domain knowledge to like,

There's this thing called retrieval augmented ⁓ generation. So you can have, you can kind of give these models a corpus of texts to kind of read and look up. There's these things, you know, and they rely on these things called embeddings, which we can talk about. And then, you know, models also have access to tools. And then another kind of big thing that a lot of the sort of agent frameworks out are like some notion of guard work rails. So like,

that they have API primitives for limiting the actions an agent can take. Besides the GitHub coding agent, have you guys used an agent that you've enjoyed or have seen an agent that you've gotten a lot of

Erika (11:27)
No, I've heard some people talk about their agents and having multi-agent workflows, like having like a test creation agent and like a coding agent and maybe like four or five different agents running in separate terminals. So that.

Jonathan Tamsut (11:27)
Yeah, okay.

Erika (11:48)
It sounds fun. I haven't gotten to that point yet.

Jonathan Tamsut (11:50)
Hehe.

Yeah.

Yeah, is, it is, yeah, I feel like we're kind of at the beginning and there isn't, you know, yeah, right now, kind of LLM based tools are super useful, but yeah, I haven't found a ton of use outside of like coding. I think like these tools are just kind of still, kind of still.

being built out.

Brittany Ellich (12:15)
Can we like take a use case? I have an idea for a use case. Can we just like walk through how you would take that and like build an agent?

Jonathan Tamsut (12:18)
Okay.

Yes,

yes. Yeah, let's do that. That sounds great.

Brittany Ellich (12:24)
Okay, so I have this idea to build an agent. want this to help me. Right now I write a newsletter where I go through and read a bunch of blog posts written by software developers and sort of categorize them around the Balanced Engineer newsletter every week. Right now I'm really dependent upon RSS feeds and just getting whatever information is pushed to me through those feeds.

Jonathan Tamsut (12:48)
Mm-hmm.

Brittany Ellich (12:48)
I would love

to build an agent though that could take that list of RSS feeds and like one, go add to that list by finding more RSS feeds from other developers that are doing something similar. And two would like search the entirety of an RSS feed and find which articles are actually the most applicable to look at or most, you know.

Jonathan Tamsut (12:59)
you

Brittany Ellich (13:10)
fit those categories to look at and like what are the best ones to look at because a lot of the things that people have written like their most recent thing isn't necessarily the best thing they've written. ⁓ So I'd like to look through the body of work within somebody's blog and say like, all right, how about you read X, Y, and Z post because they seem like the best ones to look at. So how would I build that? Please tell me.

Jonathan Tamsut (13:11)
Hmm.

Mm-hmm.

Hmm.

Mm-hmm. Yeah.

That's... ⁓

Erika (13:33)
I think

the first step is like defining your categories and your metrics for success, where I'm curious how you come up with the topics that you want to fit these posts or blogs or whatever into, and then how you determine what the best fit is currently.

Brittany Ellich (13:55)
Yeah, so I actually have a list of different topics that have some examples that go along with them. So for example, one of them is technical excellence, which looks at things like engineering blogs or ADRs or case studies and things that are about actually building skills. And then I have another topic that would be communication and collaboration, where it's more about management and leadership and team dynamics and things like that.

And so I have this list of topics already. So I feel like I have that spot taken care of. And I also have an RSS link.

Jonathan Tamsut (14:27)
Mm-hmm.

Brittany Ellich (14:29)
a list of RSS feeds that I currently look at. ⁓ I could always add to it. And shout out to anybody who writes things on the internet, please send me your RSS feed and I will include it in my list. ⁓ But it's mostly a list that I've gathered over time and I'm currently subscribed to. But with that comes this very standardized way to look through this list of blog posts through RSS, which should make this theoretically pretty easy because that's already like.

Jonathan Tamsut (14:34)
Mm-hmm.

Hmm.

Mm-hmm.

Brittany Ellich (14:53)
programmatically driven and there's a way that it's set up ⁓ through XML that it should be easy to look through and look through all of the content in each blog post and find which ones are the most relevant to each of those categories.

Jonathan Tamsut (15:06)
Yeah. You know, it's interesting. So like, right. So you have ⁓ some corpus of particles, right? You know, so you have, and then like, I think there's kind of a few problems here. What is you kind of, you kind of want to cluster articles based on these topics. And I think that's like very doable. You know, I talked about this thing called like an embedding.

If you guys, so an embedding model is basically a neural network model that it, what it basically does is you can think of it plotting text in like, in some like hyperdimensional space. You you can think of like 3D space. And then, you know, so there's basically 3D space and there's a of points that correspond to like text. And you can, you know, you can embed tokens. You can also embed like full documents.

And what an embedding model does is you can train it so that points that are close to one another are similar to one another. So like if you have, you know, like the word like king and queen will be similar to one another. And then there's also kind of, there's basically like vector math you can do on, so, so, know, each one of these points you can think of as like a vector from like the origin to the point. And then you can do like basic vector math. So you can take like king minus female.

or minus male equals queen. So there's kind of vector math you can do for mapping concept semantics from one concept to another. And so embeddings are used for, you can create an embeddings model for a bunch of documents and then you can basically use vector distance to cluster them. But this is to say, I clustering documents on like,

is doable. I do think, you mentioned you want to find the highest quality articles. That I don't know how you would do. How do you rank? it how much the article is cited by other articles? Is it its relevance to... ⁓

Brittany Ellich (17:08)
I'd say

probably like the relevance to the topic. I'd probably have to build up like a list of keywords or something like that. Sort of like a search engine, I suppose, to like try to rank them together to see which one would be the most accurate. But I mean, I don't care about it being like totally accurate. I just want it to help me like parse through these and be like, yeah, this one looks like it's related to that. So.

Jonathan Tamsut (17:15)
Mm-hmm.

Yeah.

Erika (17:23)
And this seems like

This

seems like one of those self-training opportunities where you could have it iteratively improve and not necessarily like explicitly define what best is at the beginning, but give it some set of instructions and then as it gives you back input, it will learn what you consider the best.

Jonathan Tamsut (17:37)
Mm-hmm.

Mm-hmm.

Yeah.

Yeah. And I don't even... ⁓ yeah, no, I was literally about to ask the same question you were about to ask, pretty.

Brittany Ellich (17:52)
So what tools, sorry, go ahead.

Yeah,

yeah, what tools exist to do this? Is there, mean, where do you even start with something like this?

Jonathan Tamsut (18:02)
Yeah, well I know that so there are like embeddings, document embedding APIs that you can use where you basically upload the documents and then you can assign labels to the documents and it'll essentially...

create the embedding space. that's kind of like the, it's like a supervised training algorithm. So you have documents, have labels. So when you upload a new document, it'll sort of take that document and give it the kind of label and embedding space. The second thing, yeah, it's like, yeah, like determining the, yeah, I mean, so I think there are like relevance. I mean, I wonder if you could, yeah, I mean.

In terms of finding the best article, maybe this is just an embeddings problem where you categorize any article. But you said you want to find the best articles, but you said most relevant. So maybe it still is an embeddings problem where you find essentially the most relevant article. Which I think I would push back on whether that's the best way to do that. That maybe will reduce the likelihood that you'll come across articles with really novel ideas.

Brittany Ellich (19:01)
That's a good point. Yeah, I mean, I and I don't really necessarily care that it's the best ones. I just want to make sure that like I look through this list of articles and like consider the entire body of something that somebody wrote within their blog instead of just whatever is the newest thing that comes out, ⁓ whether or not it's the best doesn't necessarily matter. But maybe just categorize it into the list of categories that I already have.

Jonathan Tamsut (19:14)
Yeah.

Brittany Ellich (19:22)
⁓ so you're saying that I would need to go through and basically take a bunch of those things and categorize them myself and then feed it into a model to like take that data and get more things and try to categorize it the same way. Is that sort of the way that it works?

Jonathan Tamsut (19:34)
Yeah, mean, and caveat, don't know what I'm talking about. This is just like, if you're asking me without Googling, that would be my answer. ⁓ I would definitely, like, that seems reasonable. Yeah, like maybe some like embeddings pipeline.

Brittany Ellich (19:42)
Okay.

Erika (19:50)
It'd be interesting to see what behavior you get with maybe two or three different approaches or goals in mind. can think of, you've mentioned wanting a diverse set of opinions and viewpoints to get a holistic sense of a topic. We've talked about like best, best articles, like however you view that as like, you know, well written or readable or

like interesting and, or like, I think the third thing we talked about was like most cited. Yeah, so I'd be kind of curious to see what output you get with even two different goals in mind, like comparing training goals with what comes out of it.

Jonathan Tamsut (20:40)
Yeah, there is this like, know, so all these models use like this concept of like RLHF, if you've heard of it, like reinforcement learning through human feedback. And it's basically right, like it's when chat GPT shows you two responses and then asks you like, which one's better, right? So the model is, you know, generating output and then

you're telling it which one's better, that's kind of it's like trading loss. It knows, okay, I want to generate answers that are more similar to this. So you could do like an RLHF thing where like, I mean, this is a lot of work, where you essentially begin to rank articles that you really like, or maybe you rank all articles by how high quality they are. And maybe there is some sort of learnable...

sort of implicit structure to like what makes a good article versus a bad article.

the end.

Yeah. But yeah, so yeah, ranking, yeah, ranking, yeah, I feel like, you ranking articles. I mean, it's funny because like, this is kind of just how Google works, Like Google, yeah, Google just like ranks articles and returns them for relevancy. And, you know, I think...

Brittany Ellich (21:39)
Yeah, it is.

Jonathan Tamsut (21:47)
There's a bunch of ways relevancy can be scored. This is kind of like the fundamental problem, like information retrieval, like giving a query and a corpus of documents, return the top K documents that are most relevant to, yeah. In which case, Google already exists. I'm not sure an agent is required. But yeah.

Brittany Ellich (22:05)
No, that's fair, but there's also

a lot of downsides, I think, to search engine optimization because a lot of that is really built on the number of keywords that are present in an article. And just because an article is stuffed with a bunch of keywords doesn't necessarily mean it's good. ⁓

Jonathan Tamsut (22:10)
Yeah.

Yeah.

Go tell

Brittany Ellich (22:20)
It's

Jonathan Tamsut (22:20)
them.

Brittany Ellich (22:21)
the same issue where, you know, if you're looking at articles that are pushed to you through something like LinkedIn or some other social platform, you're going to see whatever the most popular one is, not necessarily the one that's like the most relevant to what you're interested in. ⁓ And a lot of times those algorithms end up pushing things that are like the most, you know, the spiciest or the ones that get the most, you know.

Jonathan Tamsut (22:26)
Yeah.

Yeah.

Brittany Ellich (22:43)
negative feedback or whatever. And that's also not a thing that I really care about personally. I just want to learn from other people and what their experiences are. ⁓

Jonathan Tamsut (22:50)
Yeah.

Erika (22:52)
Yeah, it's definitely very appealing

taking back the control of the algorithm.

Jonathan Tamsut (22:57)
Yeah.

Brittany Ellich (22:58)
Yeah, that's what I want.

I just want my own algorithm. The algorithm.

Jonathan Tamsut (23:01)
Yeah, that is actually

an interesting idea. I mean, Google, I think, customizes per user. I think to an extent, what would an information retrieval system that you could truly customize and really clearly define relevancy mean? Maybe that's our next startup idea.

customized, know, customizable search engines. Although I think most people just want the same information. Or don't care. Yeah.

Brittany Ellich (23:33)
true.

Jonathan Tamsut (23:34)
Okay, so I don't know if we answered your question, but we did spend a couple of minutes talking about cool stuff.

Brittany Ellich (23:39)
Yeah, no, I think this is really interesting to think about. This is something that I haven't really spent much time thinking about, like, okay, like, what does that actually mean to build that? And it's a different set of engineering problems that I'm not super used to, but it still seems like the same fundamental building blocks of any other engineering problem.

Jonathan Tamsut (23:43)
Yeah.

Yeah.

Erika (23:57)
I think you've given me you've given me an idea for an agent to and it's not fully fleshed out, but something that works with my obsidian space to like improve my second brain, like either like suggesting edits for notes based on like, yeah, like a GitHub MCP or like

Jonathan Tamsut (24:06)
Mmm.

Erika (24:16)
Yeah, a public publicly available knowledge on the internet, like, I don't know, letting me know when things are like outdated or inaccurate or something like that and suggesting updates for my notes. Could be helpful.

Jonathan Tamsut (24:27)
Hmm. Yeah.

I, I don't know, they're pretty good.

Brittany Ellich (24:28)
I think there are some, oh, okay.

I was gonna say, I think that there are some plugins you can use that have some LLM capabilities built in that you can like feed your entire set of notes to it. But, oh, okay, what is it?

Jonathan Tamsut (24:42)
I use one. Yeah.

It is called, if I can't find it in the next four seconds, then I'm just gonna tell you. It's called Obsidian Copilot.

Erika (24:52)
Yeah, I've been wary of using any plugins because it's like proprietary data, but I do have my Obsidian in a private GitHub-owned repo, so I can play around with it there.

Jonathan Tamsut (24:58)
Yeah.

Yeah, that's fair. Yeah, this is probably, you know, in violation of some corporate policy to use. But,

Brittany Ellich (25:07)
that's smart.

I think some of them are built on like a local model though, too, which would be, I mean, as long as it's not sending data out to, you know, some server for another company somewhere.

Jonathan Tamsut (25:25)
Yeah, you could run your own llama server and use that as a backend with this, I think.

Brittany Ellich (25:31)
I do wonder if the, this is a little bit off that track, but I wonder if like the trend to having a bunch of digital notes or like the permanent knowledge management thing is going to disappear with like the move towards like digital minimalism and like not using as much like, know, data spaces you really, you know, need to use out there in the world.

Jonathan Tamsut (25:52)
Do you guys feel, do you feel pulse? Cause I don't, I don't feel.

Like, is that like an environmental thing where people are like, I don't want to, cause it's like, I don't know, like a megabyte of data, a megabyte of storage is like, fit in like my pinky. I don't know. I feel like my energy consumption is probably worse than my, for the environment that my.

Brittany Ellich (26:11)
That's true. I just read something on Blue Sky yesterday about how some person in the UK was like, you need to go delete all your old emails to free up data center space or something because it's using too much water to cool these data centers or whatever. I'm curious if that's going to be a problem.

Jonathan Tamsut (26:12)
Yeah.

Yeah.

Mm-hmm. Yeah.

Brittany Ellich (26:30)
one day. It probably isn't. Like if you're going to compare what is going to make the difference, I don't think that my personal old emails are going to make a difference compared to like the massive amount of energy used for ⁓ most corporations.

Jonathan Tamsut (26:36)
Yeah.

Yeah, but I don't know. that is that

is there's like a subreddit called like do like I think it's called like they did the math or do the math and it's where people make comments like that and then someone actually does the math. I actually am curious what the math math is like everyone's emails who are older than two years. How many terabytes of storage is that and then like how you know how you know how much carbon had to be you know emitted into the atmosphere to ever ⁓ produce that.

I think this is also a subject, but I feel like this is, we're having a wacky podcast episode, I like it. This is another subject change, but it's kind of related to agents. But I actually think one thing that's totally broken is looking for a job. And it's broken for a number of ways. I think the way we interview candidates is is totally dumb.

⁓ I also think that

Erika (27:27)
you're

experiencing this because you're going through it right now.

Jonathan Tamsut (27:30)
Yeah, yeah, yeah,

and not to yet. Yeah, but I mean, I just think, right, you know, asking like a simple algorithm challenge and then spending a few hours with a candidate, asking them standard business questions, I just feel like isn't a good predictor of whether they will be successful in that role. And I think that there's also kind of a related problem of like, I really

do believe this of like a human capitalization problem. So I think there's a lot of really smart, capable people that are just not, their skills and their intellect are not being deployed on like problems that really matter. And I think like a...

some type of omnipotent job, in the limit, if there existed some type of omnipotent job search engine that would match you with a job that you would A, like and be successful on, and C, would be a good use of your skills for society, would be an incredibly useful tool. It's sort of this social organization tool. And I wish it existed, and...

But I don't know, what are your guys' thoughts on that? There's also some maybe downsides to having a centralized thing for that. did I just hit my copy?

Brittany Ellich (28:44)
I think you just invented communism.

Erika (28:46)
Yeah, I was gonna say I fall on the complete opposite end of your perspective

where I feel like the broken part of like the broken part of hiring is not necessarily like, not necessarily like the automated part. Like it's like, it's like the people skills. Like, I feel like they're, for software engineering jobs, there's definitely like the technical aspect of like,

Jonathan Tamsut (28:53)
That's it.

Erika (29:11)
you have these skills, do you know how to do this? And like, there are ways to test for that. But I feel like the most important predictor is your like culture fit and like your personality, like, and that's 100 % not something that can be automated. Like, I think of this and like, like the answer that I would want to ask, like, the answer that I want to find as an interviewer is like, is this person resilient? Will this person grow?

Jonathan Tamsut (29:23)
Mm-hmm.

Erika (29:39)
in this role and like, you know, continue to provide value like, and like, that's not in any way something that can be.

Jonathan Tamsut (29:45)
Mm-hmm.

Erika (29:51)
answered by a machine because people change and people grow and like you're looking for those like raw human qualities and like, like I said, like the culture fit and the skills fit for the job that you're looking for right now, but you're also looking for the potential of the future. And yeah, I like, I feel like automating that to any degree is

is sort of, how do I say this, like looking at the, from my perspective, like looking at the sort of like problem, like that's not the, from my perspective, it's looking at the wrong problem.

Jonathan Tamsut (30:31)
Yeah.

Brittany Ellich (30:31)
I think also that would be expecting companies to be able to accurately define what the job is and what their culture is. I don't know if you've, I'm sure you've read some job descriptions lately. A lot of them are very grandiose compared to what the actual work is. And so that also is the other side of that. think it's expecting people to know exactly what a job is is hard, actually.

Jonathan Tamsut (30:35)
Yeah.

Yeah. Yeah. Yeah.

Erika (30:47)
Mm-hmm.

Jonathan Tamsut (30:55)
Mm-hmm.

Well, I totally agree with everything you said. Employers don't know what they want. think slope matters, then why intercept, basically. do think there's also, are really biased, and there's incredible amount of bias. There's just bias in the interview process, which hurts candidates depending on what the bias is.

Erika (31:17)
Yeah, I actually, feel like I, I'm sorry, I'm cutting you off. But I think where you might be going is like, that's maybe the problem that we want to solve is like pointing out biased decision making.

Jonathan Tamsut (31:21)
Okay, go ahead.

I don't think it's the problem, but it is a problem. like maybe, maybe, yeah, maybe it's not the most important problem.

But it is a problem, I think, ⁓ that's frustrating to experience.

Erika (31:41)
Mm-hmm.

Brittany Ellich (31:42)
I think when I think

back, sorry, when I think back to my experience too, I think that humans are also biased towards assimilating to whatever environment that they're in too. Most people I've worked with are people that I've really loved working with. I think you hear that from everybody when they leave a company, they're like, it was the people that really made this happen. maybe there's not as much risk of hiring the wrong hire as people.

Jonathan Tamsut (31:51)
Mm-hmm.

Mm-hmm.

Mm-hmm.

Brittany Ellich (32:07)
think there is. It seems like there's a lot of fear of saying like, what if I hire the wrong person? like, usually most people turn out fine.

Jonathan Tamsut (32:09)
Hmm

Well, I've heard the opposite. I've heard I've literally heard people say, you know, we've hired too many B players, you know. So, you know, I think, you know, yeah, I mean, to your point, Brittany, it's like, what is what even is a good

person at a company who knows there's not one definition companies don't even know what they want. What should a company even be doing if a company's mission is stupid then it doesn't matter if you hire the best people they're just gonna be working on a stupid thing. I mean there's just like infinite infinite turtles all the way down here but.

Erika (32:44)
That's true though. Like I do find myself reaching like even as unemployed person for like some kind of answer bank or like way to understand how leadership and like, you know, management is looking at success. Like, and like, I feel like if we're talking about, you know, agents that we would want in the world, like

that is something that I would personally find helpful, something I could go to and be like, you know, this is my like, you would do like quarterly self evaluations or whatever, right? Like, you know, stick that into an agent and be like, Hey, like, is this aligning with what my company is valuing? Like, what are some of your pointers? Because so much of our feedback comes filtered. And like,

Jonathan Tamsut (33:29)
Mm-hmm. Yeah.

Mm-hmm.

Erika (33:34)
I don't know, maybe there's like stuff happening behind the scenes, maybe there's stuff that like people wouldn't want to say to my face, but like, you know, is actually helpful for me and like performing well. So yeah, that could also be something helpful.

Jonathan Tamsut (33:43)
Yeah.

So in my in my dream at well, I'll say in this app idea I had a reverence agent. I think there is a huge career basically a personal tutor for your career thing because I do think the I'm it really excites me the idea of having AI agents as personal tutors that a have like a complete understanding of the knowledge base you want to acquire have a complete understanding of your ability and can kind of like

incrementally help you get to where you want to go. And I think it would be cool to have one for your career, like a little career agent that followed you around and was like, hey, John, you want to learn XYZ? Here are some exercises to learn XYZ, I don't know, here are some opportunities to learn XYZ, or John, you could be doing this better. I don't know. I think that would be kind of interesting, although maybe potentially annoying.

thanks so much for tuning in to Overcommitted. If you like what you hear, please do follow, subscribe, or do whatever it is you like to do on your podcast to app your choice. Check us out on Blue Sky and share with your friends. Until next week, bye-bye.

